def process_case(pipeline, device, case, video_length, sample_size, fps, shift, negative_prompt, guidance_scale, num_inference_steps, lora_weight, save_path, seed, not_use_lora, different_lora):
    generator = torch.Generator(device=device).manual_seed(seed)
    
    # 准备变量
    input_image_path_to_use = None
    # 'sample_size' (例如 832*480) 现在只作为预处理失败时的*回退*选项
    current_sample_size = sample_size 
    
    # --- [关键修改: 确保偶数 Latent] ---
    try:
        # 1. 设定约束
        max_pixels = 1280 * 720  # 约 92 万像素
        
        # [新] 动态获取 VAE 缩放因子 (很可能是 8)
        vae_scale_factor = pipeline.vae_scale_factor
        
        # [新] Transformer 需要偶数 Latent，所以像素必须是 (vae_scale_factor * 2) 的倍数
        pixel_multiple = vae_scale_factor * 2  # (例如 8 * 2 = 16)

        # 2. 加载原始图像并获取尺寸
        with Image.open(case['start_image']) as image:
            image = image.convert('RGB')
            original_width, original_height = image.size

        # 3. 计算新尺寸
        current_pixels = original_width * original_height
        
        if current_pixels > max_pixels:
            aspect_ratio = original_width / original_height
            # a. 计算缩放后的理想尺寸
            new_height = math.sqrt(max_pixels / aspect_ratio)
            new_width = new_height * aspect_ratio
        else:
            # b. 无需缩放
            new_height = original_height
            new_width = original_width

        # 4. [关键修改] 将理想尺寸四舍五入到 pixel_multiple (例如16) 的倍数
        final_height = int(round(new_height / pixel_multiple) * pixel_multiple)
        final_width = int(round(new_width / pixel_multiple) * pixel_multiple)

        # 5. 确保尺寸不为0
        if final_height == 0: final_height = pixel_multiple
        if final_width == 0: final_width = pixel_multiple

        # 6. [关键] 更新 'current_sample_size' 以便 pipeline 使用
        current_sample_size = [final_height, final_width]
        target_size_pil = (final_width, final_height)

        # 7. 将原始图像 resize 到这个最终尺寸 (无裁剪, 无黑边)
        with Image.open(case['start_image']) as image_to_resize:
             image_to_resize = image_to_resize.convert('RGB')
             processed_image = image_to_resize.resize(target_size_pil, Image.Resampling.LANCZOS)

        # 8. [不变] 保存到临时文件
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as temp_f:
            processed_image.save(temp_f.name)
            input_image_path_to_use = temp_f.name

    except Exception as e:
        print(f"警告: 图像动态分辨率处理失败 {case['start_image']}。错误: {e}")
        print(f"将回退到使用默认尺寸 {sample_size} 并可能拉伸。")
        # 回退：使用原始路径和默认的 sample_size
        input_image_path_to_use = case['start_image']
        current_sample_size = sample_size # 确保回退时尺寸正确
    # --- [修改结束] ---

    
    # [推理部分] (与上一版相同, 只是 current_sample_size 现在是16的倍数)
    try:
        if different_lora:
            # Merge LoRA
            if case.get('lora_path') and not not_use_lora:
                pipeline = merge_lora(pipeline, case['lora_path'], lora_weight, device=device)
            
            with torch.no_grad():
                latent_frames = (video_length - 1) // pipeline.vae.config.temporal_compression_ratio + 1
                
                input_video, input_video_mask, clip_image = get_image_to_video_latent(
                    input_image_path_to_use, None, video_length=video_length, 
                    sample_size=current_sample_size 
                )

                sample = pipeline(
                    case['prompt'], 
                    num_frames=video_length,
                    negative_prompt=negative_prompt,
                    height=current_sample_size[0],
                    width=current_sample_size[1],
                    generator=generator,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps,
                    video=input_video,
                    mask_video=input_video_mask,
                    clip_image=clip_image,
                    shift=shift,
                ).videos
            
            # Unmerge LoRA
            if case.get('lora_path'):
                pipeline = unmerge_lora(pipeline, case['lora_path'], lora_weight, device=device)

        else:
            with torch.no_grad():
                latent_frames = (video_length - 1) // pipeline.vae.config.temporal_compression_ratio + 1
                
                input_video, input_video_mask, clip_image = get_image_to_video_latent(
                    input_image_path_to_use, None, video_length=video_length, 
                    sample_size=current_sample_size
                )

                sample = pipeline(
                    case['prompt'], 
                    num_frames=video_length,
                    negative_prompt=negative_prompt,
                    height=current_sample_size[0],
                    width=current_sample_size[1],
                    generator=generator,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps,
                    video=input_video,
                    mask_video=input_video_mask,
                    clip_image=clip_image,
                    shift=shift,
                ).videos
        
        return sample

    finally:
        # [不变] 清理临时文件
        if input_image_path_to_use and input_image_path_to_use != case['start_image']:
            if os.path.exists(input_image_path_to_use):
                os.remove(input_image_path_to_use)
                # print(f"已删除临时文件: {input_image_path_to_use}")
